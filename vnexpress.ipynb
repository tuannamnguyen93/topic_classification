{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import fastText as ft\n",
    "import re\n",
    "import os\n",
    "config_gpu = tf.ConfigProto()\n",
    "config_gpu.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.Session(config=config_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = ft.load_model('wiki.vi.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('data_train')\n",
    "os.mkdir('data_train')\n",
    "tmp=dict()\n",
    "for i in glob.glob('data/*.txt'):\n",
    "    with open(i,'r') as f:\n",
    "        tmp[i.split('/')[-1].split('.')[0]]=[j for j in f.read().splitlines() if len(j)!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "name=dict(zip(tmp.keys(),list(range(len(tmp.keys())))))\n",
    "os.makedirs('data_train',exist_ok=True)\n",
    "# for label in tmp.keys():\n",
    "#     data=tmp[label]    \n",
    "#     i=np.random.choice(len(data),len(data),replace=False)\n",
    "#     iter_train=i[:9*len(data)//10]\n",
    "#     iter_vaild=i[9*len(data)//10:]\n",
    "#     for j in iter_train:        \n",
    "#         sentences=re.split(r'(?<!\\d)\\.(?!\\d)',data[j])\n",
    "#         if sentences[-1]=='':\n",
    "#             del(sentences[-1])        \n",
    "#         for s in range(len(sentences)):\n",
    "#             words_vecor=list()\n",
    "#             for w in sentences[s].split(' '):\n",
    "#                 w=re.compile('[\",!?\\[\\]\\(\\)]-_=').sub('',w)\n",
    "#                 words_vecor.append(word.get_word_vector(w))\n",
    "#             sentences[s]=np.array(words_vecor)\n",
    "#         data_train.append(sentences)\n",
    "#         label_train.append(name[label])\n",
    "#     for j in iter_vaild:        \n",
    "#         sentences=re.split(r'(?<!\\d)\\.(?!\\d)',data[j])\n",
    "#         if sentences[-1]=='':\n",
    "#             del(sentences[-1])        \n",
    "#         for s in range(len(sentences)):\n",
    "#             words_vecor=list()\n",
    "#             for w in sentences[s].split(' '):\n",
    "#                 w=re.compile('[\",!?\\[\\]\\(\\)]-_=').sub('',w)\n",
    "#                 words_vecor.append(word.get_word_vector(w))\n",
    "#             sentences[s]=np.array(words_vecor)\n",
    "#         data_vaild.append(sentences)\n",
    "#         label_vaild.append(name[label])\n",
    "d_train=list()\n",
    "l_train=list()\n",
    "d_vaild=list()\n",
    "l_vaild=list()\n",
    "for label in tmp.keys():\n",
    "    data=tmp[label]    \n",
    "    i=np.random.choice(len(data),len(data),replace=False)\n",
    "    iter_train=i[:9*len(data)//10]\n",
    "    iter_vaild=i[9*len(data)//10:]\n",
    "\n",
    "    for j in iter_train:        \n",
    "#         sentences=[s for s in re.split(r'(?<!\\d)\\.(?!\\d)',data[j]) if s!='']\n",
    "        words=re.sub(' +',' ',re.compile('[^\\w0-9 ]').sub('',data[j])).split(' ')\n",
    "        d_train.append(np.array([word.get_word_vector(i) for i in words]))\n",
    "        l_train.append(name[label])\n",
    "\n",
    "    for j in iter_vaild:        \n",
    "        words=re.sub(' +',' ',re.compile('[^\\w0-9 ]').sub('',data[j])).split(' ')\n",
    "        d_vaild.append(np.array([word.get_word_vector(i) for i in words]))\n",
    "        l_vaild.append(name[label])\n",
    "np.save('data_train/train',[d_train,l_train])\n",
    "np.save('data_train/vaild',[d_vaild,l_vaild])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,label_train=np.load('data_train/train.npy')\n",
    "data_vaild,label_vaild=np.load('data_train/vaild.npy')\n",
    "len_train=np.argsort([i.shape[0] for i in data_train])\n",
    "len_vaild=np.argsort([i.shape[0] for i in data_vaild])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters# Train \n",
    "starter_learning_rate = 0.001\n",
    "max_epoch = 200\n",
    "batch_size=50\n",
    "num_class=len(glob.glob('data/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/virtualenv/lstm/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "length=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "global_step = tf.placeholder(tf.int32)\n",
    "X =tf.placeholder(dtype=tf.float32,shape=[None,None,300])\n",
    "Y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "def RNN(X,numunit,name,output_dim):\n",
    "    with tf.variable_scope(name):\n",
    "#         layers=[tf.nn.rnn_cell.BasicLSTMCell(n) for n in numunit]\n",
    "#         lstm_cell =tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(num_units=numunit[-1])\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, X,initial_state=lstm_cell.zero_state(batch_size=tf.shape(X)[0],dtype=tf.float32), dtype=tf.float32)\n",
    "        index=tf.range(0,tf.shape(X)[0])*tf.shape(X)[1]+(length-1)        \n",
    "        flat=tf.reshape(outputs,[-1,numunit[-1]])\n",
    "        relevant=tf.nn.relu(tf.gather(flat,index))\n",
    "        w=tf.get_variable(name='weight',shape=[numunit[-1],output_dim],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        b=tf.get_variable(name='bias',shape=[output_dim],initializer=tf.constant_initializer(0.1),dtype=tf.float32)\n",
    "        return(tf.matmul(relevant, w) + b)\n",
    "#         return relevant\n",
    "\n",
    "logits=RNN(X,[100,150],'sentence_vector',num_class)\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=tf.one_hot(Y,num_class))\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 20, 0.96)\n",
    "optimize=tf.train.AdamOptimizer(learning_rate=starter_learning_rate).minimize(loss)\n",
    "step = 0\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "checkpoint_dir='model'\n",
    "try:\n",
    "    checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    saver.restore(sess, checkpoint)\n",
    "    files = os.listdir(checkpoint_dir)\n",
    "    for file in files:\n",
    "        if 'data' in file:\n",
    "            if int(file.split('.')[0].split('-')[-1]) > step:\n",
    "                step = int(file.split('.')[0].split('-')[-1])\n",
    "    step = step + 1\n",
    "    print('Load model Complete at step: {}'.format(step))\n",
    "except Exception:\n",
    "    print('Load model fail')\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model-149\n",
      "Load model Complete at step: 150\n",
      "Epoch 150: loss 0.4215158 (33, 2709, 300)\n",
      "\n",
      "Epoch 151: loss 0.2447007 (33, 2709, 300)\n",
      "\n",
      "Epoch 152: loss 0.2220982 (33, 2709, 300)\n",
      "\n",
      "Epoch 153: loss 0.1433384 (33, 2709, 300)\n",
      "\n",
      "Epoch 154: loss 0.0908898 (33, 2709, 300)\n",
      "\n",
      "Epoch 155: loss 0.0549598 (33, 2709, 300)\n",
      "\n",
      "Epoch 156: loss 0.0383015 (33, 2709, 300)\n",
      "\n",
      "Epoch 157: loss 0.0284213 (33, 2709, 300)\n",
      "\n",
      "Epoch 158: loss 0.0235470 (33, 2709, 300)\n",
      "\n",
      "Epoch 159: loss 0.0206097 (33, 2709, 300)\n",
      "\n",
      "in train:  0.99523261548578\n",
      "in vaild:  0.7408491947291361\n",
      "Epoch 160: loss 0.0181543 (33, 2709, 300)\n",
      "\n",
      "Epoch 161: loss 0.0158842 (33, 2709, 300)\n",
      "\n",
      "Epoch 162: loss 0.0149060 (33, 2709, 300)\n",
      "\n",
      "Epoch 163: loss 0.0137766 (33, 2709, 300)\n",
      "\n",
      "Epoch 164: loss 0.0125735 (33, 2709, 300)\n",
      "\n",
      "Epoch 165: loss 0.0117956 (33, 2709, 300)\n",
      "\n",
      "Epoch 166: loss 0.0115199 (33, 2709, 300)\n",
      "\n",
      "Epoch 167: loss 0.0110112 (33, 2709, 300)\n",
      "\n",
      "Epoch 168: loss 0.0108830 (33, 2709, 300)\n",
      "\n",
      "Epoch 169: loss 0.0104916 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.7481698389458272\n",
      "Epoch 170: loss 0.0102374 (33, 2709, 300)\n",
      "\n",
      "Epoch 171: loss 0.0099427 (33, 2709, 300)\n",
      "\n",
      "Epoch 172: loss 0.0098013 (33, 2709, 300)\n",
      "\n",
      "Epoch 173: loss 0.0097563 (33, 2709, 300)\n",
      "\n",
      "Epoch 174: loss 0.0099064 (33, 2709, 300)\n",
      "\n",
      "Epoch 175: loss 0.0102904 (33, 2709, 300)\n",
      "\n",
      "Epoch 176: loss 0.0101835 (33, 2709, 300)\n",
      "\n",
      "Epoch 177: loss 0.0125127 (33, 2709, 300)\n",
      "\n",
      "Epoch 178: loss 0.1255870 (33, 2709, 300)\n",
      "\n",
      "Epoch 179: loss 0.4219193 (33, 2709, 300)\n",
      "\n",
      "in train:  0.8009205983889528\n",
      "in vaild:  0.6764275256222547\n",
      "Epoch 180: loss 0.4001517 (33, 2709, 300)\n",
      "\n",
      "Epoch 181: loss 0.1944624 (33, 2709, 300)\n",
      "\n",
      "Epoch 182: loss 0.2198531 (33, 2709, 300)\n",
      "\n",
      "Epoch 183: loss 0.2295431 (33, 2709, 300)\n",
      "\n",
      "Epoch 184: loss 0.3955834 (33, 2709, 300)\n",
      "\n",
      "Epoch 185: loss 0.8110253 (33, 2709, 300)\n",
      "\n",
      "Epoch 186: loss 0.4839111 (33, 2709, 300)\n",
      "\n",
      "Epoch 187: loss 0.3365338 (33, 2709, 300)\n",
      "\n",
      "Epoch 188: loss 0.2490441 (33, 2709, 300)\n",
      "\n",
      "Epoch 189: loss 0.2027048 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9406542824264343\n",
      "in vaild:  0.7452415812591509\n",
      "Epoch 190: loss 0.1706314 (33, 2709, 300)\n",
      "\n",
      "Epoch 191: loss 0.1437408 (33, 2709, 300)\n",
      "\n",
      "Epoch 192: loss 0.1203615 (33, 2709, 300)\n",
      "\n",
      "Epoch 193: loss 0.1031347 (33, 2709, 300)\n",
      "\n",
      "Epoch 194: loss 0.0900756 (33, 2709, 300)\n",
      "\n",
      "Epoch 195: loss 0.0792010 (33, 2709, 300)\n",
      "\n",
      "Epoch 196: loss 0.0689899 (33, 2709, 300)\n",
      "\n",
      "Epoch 197: loss 0.0623962 (33, 2709, 300)\n",
      "\n",
      "Epoch 198: loss 0.0599540 (33, 2709, 300)\n",
      "\n",
      "Epoch 199: loss 0.0653459 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9881637349991781\n",
      "in vaild:  0.7569546120058566\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(step, (step + max_epoch) // max_epoch * max_epoch):\n",
    "    L=0\n",
    "    num=0\n",
    "    for i in np.split(len_train,range(batch_size,len(data_train),batch_size)):\n",
    "        data=data_train[i]\n",
    "        label=label_train[i]\n",
    "        le=[len(d) for d in data]\n",
    "        data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "        _,l=sess.run([optimize,loss],feed_dict={X:data,Y:label,length:le,global_step:epoch})\n",
    "        num+=len(l)\n",
    "        L+=sum(l)\n",
    "        print('Epoch {}: loss {:.7f} {}'.format(epoch,L/num,data.shape),end='\\r')\n",
    "    print('\\n')\n",
    "    if  ((epoch+1)%10==0):\n",
    "        true=0\n",
    "        for i in np.split(len_train,range(batch_size,len(data_train),batch_size)):\n",
    "            data=data_train[i]\n",
    "            label=label_train[i]\n",
    "            le=[len(d) for d in data]\n",
    "            data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "            predict=sess.run(tf.nn.softmax(logits),feed_dict={X:data,Y:label,length:le})\n",
    "            true+=np.sum(np.argmax(predict,axis=1)==label)\n",
    "        print('in train: ',true/data_train.shape[0])\n",
    "        true=0\n",
    "        for i in np.split(len_vaild,range(batch_size,len(data_vaild),batch_size)):\n",
    "            data=data_vaild[i]\n",
    "            label=label_vaild[i]\n",
    "            le=[len(d) for d in data]\n",
    "            data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "            predict=sess.run(tf.nn.softmax(logits),feed_dict={X:data,Y:label,length:le})\n",
    "            true+=np.sum(np.argmax(predict,axis=1)==label)\n",
    "        print('in vaild: ',true/data_vaild.shape[0])\n",
    "        saver.save(sess, checkpoint_dir + '/model', global_step=epoch)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "global_step = tf.placeholder(tf.int32)\n",
    "X =tf.placeholder(dtype=tf.float32,shape=[None,None,300])\n",
    "Y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "def RNN(X,numunit,name,output_dim):\n",
    "    with tf.variable_scope(name):\n",
    "        layers=[tf.nn.rnn_cell.BasicLSTMCell(n) for n in numunit]\n",
    "        lstm_cell =tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        outputs, states = tf.nn.dynamic_rnn(lstm_cell, X,initial_state=lstm_cell.zero_state(batch_size=tf.shape(X)[0],dtype=tf.float32), dtype=tf.float32)\n",
    "        index=tf.range(0,tf.shape(X)[0])*tf.shape(X)[1]+(length-1)        \n",
    "        flat=tf.reshape(outputs,[-1,numunit[-1]])\n",
    "        relevant=tf.nn.tanh(tf.gather(flat,index))\n",
    "        w=tf.get_variable(name='weight',shape=[numunit[-1],output_dim],initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32)\n",
    "        b=tf.get_variable(name='bias',shape=[output_dim],initializer=tf.constant_initializer(0.1),dtype=tf.float32)\n",
    "        return(tf.matmul(relevant, w) + b)\n",
    "#         return relevant\n",
    "\n",
    "logits=RNN(X,[100,150],'sentence_vector',num_class)\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=tf.one_hot(Y,num_class))\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 20, 0.96)\n",
    "optimize=tf.train.AdamOptimizer(learning_rate=starter_learning_rate).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "step = 0\n",
    "saver = tf.train.Saver()\n",
    "checkpoint_dir='model-2layer'\n",
    "try:\n",
    "    checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    saver.restore(sess, checkpoint)\n",
    "    files = os.listdir(checkpoint_dir)\n",
    "    for file in files:\n",
    "        if 'data' in file:\n",
    "            if int(file.split('.')[0].split('-')[-1]) > step:\n",
    "                step = int(file.split('.')[0].split('-')[-1])\n",
    "    step = step + 1\n",
    "    print('Load model Complete at step: {}'.format(step))\n",
    "except Exception:\n",
    "    print('Load model fail')\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sysadmin/virtualenv/lstm/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model-2layer/model-29\n",
      "Load model Complete at step: 30\n",
      "Epoch 30: loss 0.1542258 (33, 2709, 300)\n",
      "\n",
      "Epoch 31: loss 0.1363260 (33, 2709, 300)\n",
      "\n",
      "Epoch 32: loss 0.1292444 (33, 2709, 300)\n",
      "\n",
      "Epoch 33: loss 0.1009403 (33, 2709, 300)\n",
      "\n",
      "Epoch 34: loss 0.0883385 (33, 2709, 300)\n",
      "\n",
      "Epoch 35: loss 0.0902528 (33, 2709, 300)\n",
      "\n",
      "Epoch 36: loss 0.0690480 (33, 2709, 300)\n",
      "\n",
      "Epoch 37: loss 0.0604337 (33, 2709, 300)\n",
      "\n",
      "Epoch 38: loss 0.0690818 (33, 2709, 300)\n",
      "\n",
      "Epoch 39: loss 0.0388751 (33, 2709, 300)\n",
      "\n",
      "in train:  0.958737465066579\n",
      "in vaild:  0.7906295754026355\n",
      "Epoch 40: loss 0.0317109 (33, 2709, 300)\n",
      "\n",
      "Epoch 41: loss 0.0277658 (33, 2709, 300)\n",
      "\n",
      "Epoch 42: loss 0.0289027 (33, 2709, 300)\n",
      "\n",
      "Epoch 43: loss 0.4864684 (33, 2709, 300)\n",
      "\n",
      "Epoch 44: loss 0.8571459 (33, 2709, 300)\n",
      "\n",
      "Epoch 45: loss 0.3733282 (33, 2709, 300)\n",
      "\n",
      "Epoch 46: loss 0.1956804 (33, 2709, 300)\n",
      "\n",
      "Epoch 47: loss 0.1251460 (33, 2709, 300)\n",
      "\n",
      "Epoch 48: loss 0.0796791 (33, 2709, 300)\n",
      "\n",
      "Epoch 49: loss 0.0597880 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9130363307578497\n",
      "in vaild:  0.7584187408491947\n",
      "Epoch 50: loss 0.0502510 (33, 2709, 300)\n",
      "\n",
      "Epoch 51: loss 0.0464311 (33, 2709, 300)\n",
      "\n",
      "Epoch 52: loss 0.0340394 (33, 2709, 300)\n",
      "\n",
      "Epoch 53: loss 0.0266242 (33, 2709, 300)\n",
      "\n",
      "Epoch 54: loss 0.0234968 (33, 2709, 300)\n",
      "\n",
      "Epoch 55: loss 0.0250501 (33, 2709, 300)\n",
      "\n",
      "Epoch 56: loss 0.0227996 (33, 2709, 300)\n",
      "\n",
      "Epoch 57: loss 0.0190779 (33, 2709, 300)\n",
      "\n",
      "Epoch 58: loss 0.0184859 (33, 2709, 300)\n",
      "\n",
      "Epoch 59: loss 0.0206493 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9801084990958409\n",
      "in vaild:  0.8038067349926794\n",
      "Epoch 60: loss 0.0253675 (33, 2709, 300)\n",
      "\n",
      "Epoch 61: loss 0.0302180 (33, 2709, 300)\n",
      "\n",
      "Epoch 62: loss 0.0251151 (33, 2709, 300)\n",
      "\n",
      "Epoch 63: loss 0.0319659 (33, 2709, 300)\n",
      "\n",
      "Epoch 64: loss 0.0328764 (33, 2709, 300)\n",
      "\n",
      "Epoch 65: loss 0.0216844 (33, 2709, 300)\n",
      "\n",
      "Epoch 66: loss 0.0176566 (33, 2709, 300)\n",
      "\n",
      "Epoch 67: loss 0.0218165 (33, 2709, 300)\n",
      "\n",
      "Epoch 68: loss 0.0241744 (33, 2709, 300)\n",
      "\n",
      "Epoch 69: loss 0.0159334 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9852046687489725\n",
      "in vaild:  0.7920937042459737\n",
      "Epoch 70: loss 0.0153987 (33, 2709, 300)\n",
      "\n",
      "Epoch 71: loss 0.0212232 (33, 2709, 300)\n",
      "\n",
      "Epoch 72: loss 0.0223036 (33, 2709, 300)\n",
      "\n",
      "Epoch 73: loss 0.0136428 (33, 2709, 300)\n",
      "\n",
      "Epoch 74: loss 0.0202284 (33, 2709, 300)\n",
      "\n",
      "Epoch 75: loss 0.0213273 (33, 2709, 300)\n",
      "\n",
      "Epoch 76: loss 0.0174400 (33, 2709, 300)\n",
      "\n",
      "Epoch 77: loss 0.0155372 (33, 2709, 300)\n",
      "\n",
      "Epoch 78: loss 0.0142652 (33, 2709, 300)\n",
      "\n",
      "Epoch 79: loss 0.0146641 (33, 2709, 300)\n",
      "\n",
      "in train:  0.987012987012987\n",
      "in vaild:  0.7935578330893118\n",
      "Epoch 80: loss 0.0126593 (33, 2709, 300)\n",
      "\n",
      "Epoch 81: loss 0.0234592 (33, 2709, 300)\n",
      "\n",
      "Epoch 82: loss 0.0683150 (33, 2709, 300)\n",
      "\n",
      "Epoch 83: loss 0.0529875 (33, 2709, 300)\n",
      "\n",
      "Epoch 84: loss 0.2678340 (33, 2709, 300)\n",
      "\n",
      "Epoch 85: loss 0.0905127 (33, 2709, 300)\n",
      "\n",
      "Epoch 86: loss 0.1401535 (33, 2709, 300)\n",
      "\n",
      "Epoch 87: loss 0.0500706 (33, 2709, 300)\n",
      "\n",
      "Epoch 88: loss 0.0247799 (33, 2709, 300)\n",
      "\n",
      "Epoch 89: loss 0.0196413 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9876705572908104\n",
      "in vaild:  0.7964860907759883\n",
      "Epoch 90: loss 0.0152377 (33, 2709, 300)\n",
      "\n",
      "Epoch 91: loss 0.0126121 (33, 2709, 300)\n",
      "\n",
      "Epoch 92: loss 0.0112061 (33, 2709, 300)\n",
      "\n",
      "Epoch 93: loss 0.0108661 (33, 2709, 300)\n",
      "\n",
      "Epoch 94: loss 0.0103177 (33, 2709, 300)\n",
      "\n",
      "Epoch 95: loss 0.0129650 (33, 2709, 300)\n",
      "\n",
      "Epoch 96: loss 0.0119777 (33, 2709, 300)\n",
      "\n",
      "Epoch 97: loss 0.0107987 (33, 2709, 300)\n",
      "\n",
      "Epoch 98: loss 0.0101788 (33, 2709, 300)\n",
      "\n",
      "Epoch 99: loss 0.0103403 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9866842018740752\n",
      "in vaild:  0.8052708638360175\n",
      "Epoch 100: loss 0.0136831 (33, 2709, 300)\n",
      "\n",
      "Epoch 101: loss 0.0106149 (33, 2709, 300)\n",
      "\n",
      "Epoch 102: loss 0.0096418 (33, 2709, 300)\n",
      "\n",
      "Epoch 103: loss 0.0092171 (33, 2709, 300)\n",
      "\n",
      "Epoch 104: loss 0.0097690 (33, 2709, 300)\n",
      "\n",
      "Epoch 105: loss 0.0091592 (33, 2709, 300)\n",
      "\n",
      "Epoch 106: loss 0.0092130 (33, 2709, 300)\n",
      "\n",
      "Epoch 107: loss 0.0088184 (33, 2709, 300)\n",
      "\n",
      "Epoch 108: loss 0.0088856 (33, 2709, 300)\n",
      "\n",
      "Epoch 109: loss 0.0086544 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8023426061493412\n",
      "Epoch 110: loss 0.0086109 (33, 2709, 300)\n",
      "\n",
      "Epoch 111: loss 0.0086026 (33, 2709, 300)\n",
      "\n",
      "Epoch 112: loss 0.0087461 (33, 2709, 300)\n",
      "\n",
      "Epoch 113: loss 0.0084331 (33, 2709, 300)\n",
      "\n",
      "Epoch 114: loss 0.0085279 (33, 2709, 300)\n",
      "\n",
      "Epoch 115: loss 0.0086762 (33, 2709, 300)\n",
      "\n",
      "Epoch 116: loss 0.0088480 (33, 2709, 300)\n",
      "\n",
      "Epoch 117: loss 0.0086597 (33, 2709, 300)\n",
      "\n",
      "Epoch 118: loss 0.0085149 (33, 2709, 300)\n",
      "\n",
      "Epoch 119: loss 0.0088752 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8008784773060029\n",
      "Epoch 120: loss 0.0913139 (33, 2709, 300)\n",
      "\n",
      "Epoch 121: loss 0.1035410 (33, 2709, 300)\n",
      "\n",
      "Epoch 122: loss 0.0412574 (33, 2709, 300)\n",
      "\n",
      "Epoch 123: loss 0.0238450 (33, 2709, 300)\n",
      "\n",
      "Epoch 124: loss 0.0232189 (33, 2709, 300)\n",
      "\n",
      "Epoch 125: loss 0.0193644 (33, 2709, 300)\n",
      "\n",
      "Epoch 126: loss 0.0119938 (33, 2709, 300)\n",
      "\n",
      "Epoch 127: loss 0.0101306 (33, 2709, 300)\n",
      "\n",
      "Epoch 128: loss 0.0092473 (33, 2709, 300)\n",
      "\n",
      "Epoch 129: loss 0.0087186 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9953970080552359\n",
      "in vaild:  0.8096632503660323\n",
      "Epoch 130: loss 0.0090267 (33, 2709, 300)\n",
      "\n",
      "Epoch 131: loss 0.0087988 (33, 2709, 300)\n",
      "\n",
      "Epoch 132: loss 0.0083154 (33, 2709, 300)\n",
      "\n",
      "Epoch 133: loss 0.0081852 (33, 2709, 300)\n",
      "\n",
      "Epoch 134: loss 0.0082199 (33, 2709, 300)\n",
      "\n",
      "Epoch 135: loss 0.0082167 (33, 2709, 300)\n",
      "\n",
      "Epoch 136: loss 0.0080642 (33, 2709, 300)\n",
      "\n",
      "Epoch 137: loss 0.0080547 (33, 2709, 300)\n",
      "\n",
      "Epoch 138: loss 0.0079253 (33, 2709, 300)\n",
      "\n",
      "Epoch 139: loss 0.0079521 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8257686676427526\n",
      "Epoch 140: loss 0.0079594 (33, 2709, 300)\n",
      "\n",
      "Epoch 141: loss 0.0081277 (33, 2709, 300)\n",
      "\n",
      "Epoch 142: loss 0.0081484 (33, 2709, 300)\n",
      "\n",
      "Epoch 143: loss 0.0083978 (33, 2709, 300)\n",
      "\n",
      "Epoch 144: loss 0.0083735 (33, 2709, 300)\n",
      "\n",
      "Epoch 145: loss 0.0085098 (33, 2709, 300)\n",
      "\n",
      "Epoch 146: loss 0.0084219 (33, 2709, 300)\n",
      "\n",
      "Epoch 147: loss 0.0085458 (33, 2709, 300)\n",
      "\n",
      "Epoch 148: loss 0.0087860 (33, 2709, 300)\n",
      "\n",
      "Epoch 149: loss 0.0083646 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8199121522693997\n",
      "Epoch 150: loss 0.0080752 (33, 2709, 300)\n",
      "\n",
      "Epoch 151: loss 0.0078475 (33, 2709, 300)\n",
      "\n",
      "Epoch 152: loss 0.0078208 (33, 2709, 300)\n",
      "\n",
      "Epoch 153: loss 0.0078557 (33, 2709, 300)\n",
      "\n",
      "Epoch 154: loss 0.0078862 (33, 2709, 300)\n",
      "\n",
      "Epoch 155: loss 0.0079922 (33, 2709, 300)\n",
      "\n",
      "Epoch 156: loss 0.0080884 (33, 2709, 300)\n",
      "\n",
      "Epoch 157: loss 0.0079800 (33, 2709, 300)\n",
      "\n",
      "Epoch 158: loss 0.0079624 (33, 2709, 300)\n",
      "\n",
      "Epoch 159: loss 0.0079504 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8199121522693997\n",
      "Epoch 160: loss 0.0079163 (33, 2709, 300)\n",
      "\n",
      "Epoch 161: loss 0.0082884 (33, 2709, 300)\n",
      "\n",
      "Epoch 162: loss 0.0080861 (33, 2709, 300)\n",
      "\n",
      "Epoch 163: loss 0.1181696 (33, 2709, 300)\n",
      "\n",
      "Epoch 164: loss 0.2123152 (33, 2709, 300)\n",
      "\n",
      "Epoch 165: loss 0.0550490 (33, 2709, 300)\n",
      "\n",
      "Epoch 166: loss 0.0204459 (33, 2709, 300)\n",
      "\n",
      "Epoch 167: loss 0.0134503 (33, 2709, 300)\n",
      "\n",
      "Epoch 168: loss 0.0113812 (33, 2709, 300)\n",
      "\n",
      "Epoch 169: loss 0.0097739 (33, 2709, 300)\n",
      "\n",
      "in train:  0.99523261548578\n",
      "in vaild:  0.7994143484626647\n",
      "Epoch 170: loss 0.0090845 (33, 2709, 300)\n",
      "\n",
      "Epoch 171: loss 0.0085065 (33, 2709, 300)\n",
      "\n",
      "Epoch 172: loss 0.0082445 (33, 2709, 300)\n",
      "\n",
      "Epoch 173: loss 0.0080881 (33, 2709, 300)\n",
      "\n",
      "Epoch 174: loss 0.0079656 (33, 2709, 300)\n",
      "\n",
      "Epoch 175: loss 0.0078983 (33, 2709, 300)\n",
      "\n",
      "Epoch 176: loss 0.0078434 (33, 2709, 300)\n",
      "\n",
      "Epoch 177: loss 0.0078092 (33, 2709, 300)\n",
      "\n",
      "Epoch 178: loss 0.0078006 (33, 2709, 300)\n",
      "\n",
      "Epoch 179: loss 0.0078571 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8199121522693997\n",
      "Epoch 180: loss 0.0079839 (33, 2709, 300)\n",
      "\n",
      "Epoch 181: loss 0.0080077 (33, 2709, 300)\n",
      "\n",
      "Epoch 182: loss 0.0079029 (33, 2709, 300)\n",
      "\n",
      "Epoch 183: loss 0.0079294 (33, 2709, 300)\n",
      "\n",
      "Epoch 184: loss 0.0079930 (33, 2709, 300)\n",
      "\n",
      "Epoch 185: loss 0.0080037 (33, 2709, 300)\n",
      "\n",
      "Epoch 186: loss 0.0079117 (33, 2709, 300)\n",
      "\n",
      "Epoch 187: loss 0.0079024 (33, 2709, 300)\n",
      "\n",
      "Epoch 188: loss 0.0078248 (33, 2709, 300)\n",
      "\n",
      "Epoch 189: loss 0.0078322 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8199121522693997\n",
      "Epoch 190: loss 0.0078090 (33, 2709, 300)\n",
      "\n",
      "Epoch 191: loss 0.0078011 (33, 2709, 300)\n",
      "\n",
      "Epoch 192: loss 0.0077164 (33, 2709, 300)\n",
      "\n",
      "Epoch 193: loss 0.0076935 (33, 2709, 300)\n",
      "\n",
      "Epoch 194: loss 0.0076933 (33, 2709, 300)\n",
      "\n",
      "Epoch 195: loss 0.0077029 (33, 2709, 300)\n",
      "\n",
      "Epoch 196: loss 0.0078111 (33, 2709, 300)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197: loss 0.0079539 (33, 2709, 300)\n",
      "\n",
      "Epoch 198: loss 0.0080423 (33, 2709, 300)\n",
      "\n",
      "Epoch 199: loss 0.0079484 (33, 2709, 300)\n",
      "\n",
      "in train:  0.9955614006246918\n",
      "in vaild:  0.8184480234260615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(step, (step + max_epoch) // max_epoch * max_epoch):\n",
    "    L=0\n",
    "    num=0\n",
    "    for i in np.split(len_train,range(batch_size,len(data_train),batch_size)):\n",
    "        data=data_train[i]\n",
    "        label=label_train[i]\n",
    "        le=[len(d) for d in data]\n",
    "        data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "        _,l=sess.run([optimize,loss],feed_dict={X:data,Y:label,length:le,global_step:epoch})\n",
    "        num+=len(l)\n",
    "        L+=sum(l)\n",
    "        print('Epoch {}: loss {:.7f} {}'.format(epoch,L/num,data.shape),end='\\r')\n",
    "    print('\\n')\n",
    "    if  ((epoch+1)%10==0):\n",
    "        true=0\n",
    "        for i in np.split(len_train,range(batch_size,len(data_train),batch_size)):\n",
    "            data=data_train[i]\n",
    "            label=label_train[i]\n",
    "            le=[len(d) for d in data]\n",
    "            data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "            predict=sess.run(tf.nn.softmax(logits),feed_dict={X:data,Y:label,length:le})\n",
    "            true+=np.sum(np.argmax(predict,axis=1)==label)\n",
    "        print('in train: ',true/data_train.shape[0])\n",
    "        true=0\n",
    "        for i in np.split(len_vaild,range(batch_size,len(data_vaild),batch_size)):\n",
    "            data=data_vaild[i]\n",
    "            label=label_vaild[i]\n",
    "            le=[len(d) for d in data]\n",
    "            data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "            predict=sess.run(tf.nn.softmax(logits),feed_dict={X:data,Y:label,length:le})\n",
    "            true+=np.sum(np.argmax(predict,axis=1)==label)\n",
    "        print('in vaild: ',true/data_vaild.shape[0])\n",
    "        saver.save(sess, checkpoint_dir + '/model', global_step=epoch)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=dict()\n",
    "def test(strings):\n",
    "    for i in glob.glob('data/*.txt'):\n",
    "        tmp[len(tmp)]=i.split('/')[-1].split('.')[0]\n",
    "    for i in strings:\n",
    "        words=re.sub(' +',' ',re.compile('[^\\w0-9 ]').sub('',data[j])).split(' ')\n",
    "        data.append(np.array([word.get_word_vector(j) for j in words]))\n",
    "    le=[len(d) for d in data]\n",
    "    data=np.array([np.pad(i,[[0,max(le)-len(i)],[0,0]],'constant') for i in data])\n",
    "    predict=sess.run(tf.argmax(tf.nn.softmax(logits)),feed_dict={X:data,length:le})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
